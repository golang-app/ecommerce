version: '3.7'

services:
  frontend:
    build:
      context: ./frontend
    ports:
      - 3000:3000
    networks:
      - ecommerce
  backend:
    build:
      context: ./backend
    restart: unless-stopped
    depends_on:
      - migrate
      - logstash
    links:
      - postgres
      - otelcol
      - logstash
    ports:
      - 8080:8080
    networks:
      - ecommerce
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_HOST: postgres
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otelcol:4318/
  postgres:
    image: postgres
    restart: always
    ports:
      - 5432:5432
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ecommerce
    networks:
      - ecommerce
  migrate:
    image: migrate/migrate
    restart: on-failure
    depends_on:
      - postgres
    links:
      - postgres
    volumes:
      - ./migrations:/migrations
    networks:
      - ecommerce
    command: ["-path", "/migrations", "-database",  "postgres://postgres:postgres@postgres:5432/ecommerce?sslmode=disable", "up"]

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "6831:6831/udp"
      - "16686:16686" # UI
      - "4317"  # gRPC
    networks:
      - ecommerce
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus

  grafana:
    image: grafana/grafana:latest
    depends_on:
      - jaeger
      - prometheus
    volumes:
      - ./.docker/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./.docker/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./.docker/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3001:3000"
    networks:
      - ecommerce

  otelcol:
    image: otel/opentelemetry-collector-contrib:latest
    deploy:
      resources:
        limits:
          memory: 100M
    restart: always
    command:
      - --config=/etc/otelcol-config.yml
    volumes:
      - ./.docker/collector-config.yml:/etc/otelcol-config.yml
    ports:
      - "4317"          # OTLP over gRPC receiver
      - "4318:4318"     # OTLP over HTTP receiver
      - "9464"          # Prometheus exporter
      - "8888"          # metrics endpoint
    networks:
      - ecommerce
    depends_on:
      - jaeger

  prometheus:
    image: quay.io/prometheus/prometheus:latest
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
    volumes:
      - ./.docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yaml
    networks:
      - ecommerce
    ports:
      - "9091:9090"

  # The 'setup' service runs a one-off script which initializes users inside
  # Elasticsearch — such as 'logstash_internal' and 'kibana_system' — with the
  # values of the passwords defined in the '.env' file.
  #
  # This task is only performed during the *initial* startup of the stack. On all
  # subsequent runs, the service simply returns immediately, without performing
  # any modification to existing users.
  setup:
    build:
      context: .docker/setup/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    init: true
    volumes:
      - .docker/setup/entrypoint.sh:/entrypoint.sh:ro,Z
      - .docker/setup/lib.sh:/lib.sh:ro,Z
      - .docker/setup/roles:/roles:ro,Z
      - .docker/setup:/state:Z
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-}
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
      METRICBEAT_INTERNAL_PASSWORD: ${METRICBEAT_INTERNAL_PASSWORD:-}
      FILEBEAT_INTERNAL_PASSWORD: ${FILEBEAT_INTERNAL_PASSWORD:-}
      HEARTBEAT_INTERNAL_PASSWORD: ${HEARTBEAT_INTERNAL_PASSWORD:-}
      MONITORING_INTERNAL_PASSWORD: ${MONITORING_INTERNAL_PASSWORD:-}
      BEATS_SYSTEM_PASSWORD: ${BEATS_SYSTEM_PASSWORD:-}
    networks:
      - ecommerce
    depends_on:
      - elasticsearch

  elasticsearch:
    build:
      context: .docker/elasticsearch/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - .docker/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,Z
      - .docker/elasticsearch:/usr/share/elasticsearch/data:Z
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      node.name: elasticsearch
      ES_JAVA_OPTS: -Xms512m -Xmx512m
      # Bootstrap password.
      # Used to initialize the keystore during the initial startup of
      # Elasticsearch. Ignored on subsequent runs.
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-}
      # Use single node discovery in order to disable production mode and avoid bootstrap checks.
      # see: https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html
      discovery.type: single-node
    networks:
      - ecommerce
    restart: unless-stopped

  logstash:
    build:
      context: .docker/logstash/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - .docker/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro,Z
      - .docker/logstash/pipeline:/usr/share/logstash/pipeline:ro,Z
    ports:
      - 5044:5044
      - 50000:50000/tcp
      - 50000:50000/udp
      - 9600:9600
    environment:
      LS_JAVA_OPTS: -Xms256m -Xmx256m
      LOGSTASH_INTERNAL_PASSWORD: ${LOGSTASH_INTERNAL_PASSWORD:-}
    networks:
      - ecommerce
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    build:
      context: .docker/kibana/
      args:
        ELASTIC_VERSION: ${ELASTIC_VERSION}
    volumes:
      - .docker/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:ro,Z
    ports:
      - 5601:5601
    environment:
      KIBANA_SYSTEM_PASSWORD: ${KIBANA_SYSTEM_PASSWORD:-}
    networks:
      - ecommerce
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  setup:
  elasticsearch:


networks:
  ecommerce:
    driver: bridge

volumes:
    prometheus_data: {}
    grafana_data: {}
    setup:
    elasticsearch:

